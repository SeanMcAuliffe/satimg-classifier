\documentclass[10pt]{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2020}
\usepackage{mathptmx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{\emph{Landsat} Image Classification using Convolutional Neural Networks}

\author{
  Sean McAuliffe,  V00913346
  \And
  Spencer Davis, V00759537 
  \And
  Kiana Pazdernik, V00896924 
  \And
  Mateo Moody, V00918050 
  \And
  Chris Wong, V00780634 
}

\begin{document}

\maketitle

\begin{abstract}
  The report describes a project that aims to use Convolutional Neural Networks
  (CNNs) to identify locations for mineral deposits on the earth's surface using
  the dataset from NASA's Landsat program and a dataset of known mineral
  deposits. The goal is to minimize the environmental impact caused by mineral
  extraction, which is harmful to the environment and human health. The Landsat
  dataset is preprocessed by downsampling the images and labeling them using a
  K-D Tree Search Algorithm. The labeled images are sorted into location buckets
  and ocean masking is performed to ensure that the CNN built is not predicting
  whether the image is land or ocean. The images at each location are then
  sorted by degree of cloud cover, allowing the selection of training images
  with fewer clouds. Once the images are preprocessed, they are split into
  training and testing sets with balanced classes for binary classification,
  or with equivalent distributions for regression. Two types of CNN models,
  binary classification, and regression, were implemented to determine the
  land's suitability for mineral extraction. Several different model
  architectures were investigated to achieve high accuracy, and a preliminary
  baseline was obtained for evaluation purposes. The trained models obtained an
  accuracy of 73\% on the binary classification problem, and a MAE of 0.117 on
  the regression problem; these values were both significant improvements on the
  baselines. Training on more powerful hardware and using the full-resolution
  images may produce further improvements.
\end{abstract}

\section{Introduction}

Convolutional Neural Networks (CNNs) have proved to be highly
effective in real-life applications, especially in image classification, as they
can extract features from patterns without being affected by their locations [1].
As such, the dataset from NASA's Landsat program contained over 3 million earth
observation scenes. This dataset was used alongside a dataset containing the
world's known mineral deposits on Earth. The goal of using CNNs to determine
locations for mineral deposits. A problem in natural resource extraction is that
it is extremely harmful to the environment and human health, however, it is
crucial for construction, manufacturing, and energy production. Many countries
generate significant revenue from mineral resource extraction, therefore it is a
field that will continue to thrive. Petroleum and gas production is required for
mineral extraction and contributes to surface and groundwater pollution,
erosion, and sedimentation. Using CNNs and the NASA Landsat dataset to identify
areas with rich mineral deposits for targeted exploration and extraction can
help minimize environmental impact.

The primary objective of the project was to create a supervised
machine learning algorithm, CNNs, that could accurately predict the land's
suitability for mineral extraction. By identifying the exact locations and types
of minerals present in the land, the amount of pollution caused by the
extraction of waste rock and the chemical process of separating the desired
minerals from the waste rock could be significantly reduced [2]. The Landsat
dataset contains extensive images of Earth, where the images were cross-referenced
for existing resource deposits to predict mineral deposit locations.

\section{Data Processing}

The Landsat dataset was preprocessed before training the CNN. The Landsat dataset
chosen was Landsat 4 - band 7, which used Thematic Mapper sensors to capture the
images. This means the images were shortwave infrared, and sensitive to
hydrothermally altered rocks, associated with mineral deposits. The images were
taken from the vertices of a 2x2 latitude-longitude grid, all approximately
7500x7500 pixels in a single colour channel. The mineral deposit dataset
comprised about 350,000 known public mineral deposits and the minerals present.
This data was verified by manual inspection of random records and is therefore
incomplete.

Figure 1: Example images from Landsat 4 Band 7.

Figure 2: Mineral deposit dataset visualization on world map. 

In the data preprocessing pipeline, the images were downsampled to allow for
training on the authorsâ€™ personal computers using consumer GPUs. Since 7500x7500
pixels are too large, the Python library ImageMagick is used to downsample the
images to 512x512 pixels. The sampling is used because it is the fastest,
however, it has the highest information loss for the images. Each image takes
approximately 6 seconds to process through the acquisition pipeline, and a total
of 75 hours are needed to process all of the images in the dataset. The metadata
file is then downloaded, which contains additional information about the images,
such as the location and other identifying details.

Figure 3: Label Creation. 

Figure 4: Step by step data preprocessing pipeline. 

A K-D Tree Search Algorithm was used to label the images according to the mineral
deposit dataset, since KD trees greatly accelerate queries on spatial data [3].
In the label creation, the deposit locations are first placed in a KD tree. Then
for each image, the enclosing circle of the image is obtained, the KD tree is
queried for the subset of deposits present in the enclosing circle, and a convex
polygon test is used to identify which of those deposits lie within the image.
This method was used since it supported queries on the irregular regions shown
in Landsat images, which are neither axis-aligned nor perfectly rectangular. For
binary classification, the images are labeled 1 or 0 depending on the presence
or absence of mineral deposits. For regression, the images are labeled according
to a richness score in a range of 0 to 1. The richness score is computed as the
log transform of the absolute count of deposits in the image, normalized to the
range [0, 11]; the log transform was used to reduce the skew of the regression
values.

Once the images are labeled using the KD tree search algorithm, the images are
then sorted into buckets based on their locations. There are approximately 6000
unique location buckets. This allows us to ensure that images of the same
location do not end up in the training and test sets simultaneously. The images
containing the ocean are removed. To perform the ocean masking, the Python
library Geopandas is used to load the geospatial data for the borders of every
country. Each image's center point is tested for where it belongs in the polygons,
to determine where the image belongs in each country. This ensures that the CNN
built is not predicting whether the image is land or ocean. Finally, the images
at each location are sorted by average pixel brightness, which effectively sorts
by degree of cloud cover. This permits the selection of images having less cloud
cover, as the clouds can interfere with predicting if the land contains minerals.

Once the images have labels and have been preprocessed, the images are split
80/20 into training and testing sets with balanced classes for binary regression,
or equivalent distributions for regression. All pixel brightnesses were
normalized to the range [0, 1].

\section{Convolutional Neural Networks}

The chosen machine learning algorithm was a CNN using binary classification and
a CNN using regression. Tuning was performed in two stages: first the
architecture was tuned, then the hyperparameters were tuned. Several different
model architectures were investigated to achieve high accuracy. During tuning,
the models were varied over two architectural characteristics: the size of the
convolutional network and the size of the dense network. Each network was made
to be large, medium, or small, as a label for each. This helped determine
whether the problem benefited from learning highly abstract patterns, such as
deep and narrow models, or staying closer to the source data with shallow and
wide models. The architecture producing the greatest accuracy or lowest error
was then selected and subjected to hyperparameter tuning. 

The preliminary baseline, without any optimizations or hyperparameter tuning,
and with the training labels randomized, returned results for binary
classification of 50\% value accuracy, and the regression produced a mean
absolute error (MAE) of 0.127, which is a measure of the difference between the
predicted and actual values. A lower MAE indicates better accuracy in predicting
the richness score of the mineral deposits. Obtaining a baseline was an
important step in evaluating the performance of the models, as it helps determine
if any additional efforts to optimize the model are improving its performance.

The CNNs were trained on samples drawn from the unique location buckets over
land, and each with a minimum of cloud cover. For training the CNN, a test
bench was created with a setup of Tensorflow GPU support, to speed up the model
training times.

\section{Problem Definition}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
type with a vertical spacing (leading) of 11~points.  Times New Roman is the
preferred typeface throughout, and will be selected for you by default.
Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
indentation.

The formatting instructions contained in these style files are summarized in


The paper title should be 17~point, initial caps/lower case, bold, centered
between two horizontal rules. The top rule should be 4~points thick and the
bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
below the title to rules. All pages should start at 1~inch (6~picas) from the
top of the page.

For the final version, authors' names are set in boldface, and each name is
centered above the corresponding address. The lead author's name is to be listed
first (left-most), and the co-authors' names (if different address) are set to
follow. If there is only one co-author, list both author and co-author side by
side.

Please pay special attention to the instructions in Section \ref{others}
regarding figures, tables, acknowledgments, and references.

\section{Related Work}
\label{headings}

All headings should be lower case (except for first word and proper nouns),
flush left, and bold.

First-level headings should be in 12-point type.

\subsection{Headings: second level}

Second-level headings should be in 10-point type.

\subsubsection{Headings: third level}

Third-level headings should be in 10-point type.

\paragraph{Paragraphs}

There is also a \verb+\paragraph+ command available, which sets the heading in
bold, flush left, and inline with the text, with the heading followed by 1\,em
of space.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone.

\subsection{Citations within the text}

The \verb+natbib+ package will be loaded for you by default.  Citations may be
author/year or numeric, as long as you maintain internal consistency.  As to the
format of the references themselves, any style is acceptable as long as it is
used consistently.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+neurips_2020+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}

If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{neurips_2020}
\end{verbatim}

As submission is double blind, refer to your own published work in the third
person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
previous work [4].'' If you cite your other papers that are not widely available
(e.g., a journal paper under review), use anonymous author names in the
citation, e.g., an author of the form ``A.\ Anonymous.''

\subsection{Footnotes}

Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).

Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}

\subsection{Figures}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}

All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.

You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.

\subsection{Tables}

All tables must be centered, neat, clean and legible.  The table number and
title always appear before the table.  See Table~\ref{sample-table}.

Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.

Note that publication-quality tables \emph{do not contain vertical rules.} We
strongly suggest the use of the \verb+booktabs+ package, which allows for
typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.

\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Final instructions}

Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PDF files}

Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''

Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.

\begin{itemize}

\item You should directly generate PDF files using \verb+pdflatex+.

\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.

\item The IEEE has recommendations for generating PDF files whose fonts are also
  acceptable for NeurIPS. Please see
  \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.

\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.

\end{itemize}

If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.

\subsection{Margins in \LaTeX{}}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.


\section*{Broader Impact}

Authors are required to include a statement of the broader impact of their work,
including its ethical aspects and future societal consequences. 
Authors should discuss both positive and negative outcomes, if any. For instance,
authors should discuss a) 
who may benefit from this research, b) who may be put at disadvantage from this
research, c) what are the consequences of failure of the system, and d) whether
the task/method leverages biases in the data. If authors believe this is not
applicable to them, authors can simply state this.

Use unnumbered first level headings for this section, which should go at the
end of the paper. {\bf Note that this section does not count towards the eight
pages of content that are allowed.}

\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are
required to declare 
funding (financial activities supporting the submitted work) and competing
interests (related financial activities outside the submitted work). 
More information about this disclosure can be found at:
\url{https://neurips.cc/Conferences/2020/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the
final paper. You can use the \texttt{ack} environment provided in the style
file to autmoatically hide this section in the anonymized submission.
\end{ack}

\section*{References}

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
{\bf Note that the Reference section does not count towards the eight pages of
content that are allowed.}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
